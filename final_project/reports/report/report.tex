%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Article
% LaTeX Template
% Version 2.0 (28/2/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[10pt, a4paper, twocolumn]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Effective Methods for Capturing Cattle Rustlers} % The article title

\author{
	\authorstyle{Yung-Hsin Chen\textsuperscript{1} and Haoxin Cai\textsuperscript{1}} % Authors
	\newline\newline % Space before institutions
	\textsuperscript{1}\institution{Universit{\"a}t Z{\"u}rich, Z{\"u}rich, Switzerland} % Institution 1
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{19 December 2022} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{The goal of the report is to get the most related factors of COVID-19 
pandemic cases in countries. In this report, data are collected from 192 countries, and classification 
models are applied in order to get a list of feature importance. It is believed that the more-important-features 
play more crucial role in the severity of the pandemic of a certain country. With an accuracy of 74.36\%, 
XGBoost model suggests that human development index, life expectancy, population density, 
hospital beds per thousand and GDP per capita of a country are the leading factors of the severity 
of the pandemic.}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------
\section{Introduction}The COVID-19 pandemic has severe impacts on almost every aspect around the world. 
It causes not only social and economic disruption, but also drastic death rates. Inevitably, people 
are curious about the causalities of COVID-19 and how to prevent the pandemic from getting worse. 
Therefore, the report aims to determine the correlation between the severity of the pandemic 
other information of a country. Notes that the time parameters are taken out of consideration for simplicity. 

%----------------------------------------------------------------------------------------
%	DOMAIN KNOWLEDGE
%----------------------------------------------------------------------------------------
\section{Domain Knowledge}\label{sec:domain_knowledge}
In this section, tools and domain knowledges used in the task and reasons of choosing them  will be 
briefly explained including models used (logistic regression, linear perceptron, XGBoost), evaluation metrices 
(micro-f1, macro-f1) and visualisations (confusion matrix, normalised confusion matrix).
\subsection{Logistic Regression}
Logistic regression is commonly used for classification problems. It aims to fit the probability of an event 
belonging to a certain class, which can be illustrated in \autoref{eq:logits}.
\begin{equation}\label{eq:logits}
	\overbrace{log\underbrace{\left(\frac{P(X)}{1-P(X)}\right)}_\text{odd}}^\text{logit function} = \beta_0 + \beta_1X + \beta_2X^2 +...
\end{equation}
However, the logit function ranges from -inf to inf, making it sensitive to outliers like linear regression and hard to address 
the loss function. In order to output the probability of the event belonging to a certain class, the logit function is mapped 
back to the probability as shown in \autoref{eq:sigmoid}, which is also known as the Sigmoid function.
\begin{equation}\label{eq:sigmoid}
	P(X) = \frac{e^{\beta_0 + \beta_1X + \beta_2X^2 +...}}{1+e^{\beta_0 + \beta_1X + \beta_2X^2 +...}}
\end{equation}
The algorithm of logistic regression aims to fit the best Sigmoid curve for the training data by calculating gradients of the 
log likelihood function in \autoref{eq:lr_mle} by applying maximum likelihood estimation (MLE).
\begin{equation}\label{eq:lr_mle}
\begin{split}
	& \text{likelihood function} = \\
	& L(\beta) = \prod\left(p(x_i)^y_i(1-p(x_i))^{1-y_i}\right)\\
	&log(L(\beta)) = \sum_i\left(y_ilogP(x_i)+(1-y_i)(1-P(x_i))\right)
\end{split}
\end{equation}
\subsection{Linear Perceptron}
\subsection{F1-score: Micro \& Macro}
F1 score is a metric of taking precision\footnote{According to Layman definition, percision means, 
of all the positive predictions I made, how many of them are truly positive?} and 
recall\footnote{According to Layman definition, percision means, of all the actual positive examples 
out there, how many of them did I correctly predict to be positive?} into into consideration at the 
same time per class. F1-score is defined as \autoref{eq:f1_score}.\\[10pt]
\begin{equation}\label{eq:f1_score}
	\begin{split}
		\text{f1-score} &= 2\times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}\\
		&= \frac{\text{TP}}{\text{TP}+\frac{1}{2}(\text{FP+FN})}
	\end{split}
\end{equation}
Since f1-score is calculated per class, to calculate the aggregation of multi-class will become tricky. 
This is where micro-f1 and macro-f1 come into play. They are two different ways of aggregating multi-class 
f1-scores. Macro-f1 calculates the average of f1-scores of all classes as \autoref{eq:macro_f1}. The equation 
shows that macro-f1 gives same weights to each class no matter how large the class is. 
\begin{equation}\label{eq:macro_f1}
	\text{macro-f1} = \frac{\text{sum(f1-score)}}{\text{number of classes}}
\end{equation}
On the other hand, micro-f1 is defined by \autoref{eq:micro_f1}. This equation is the same as the one for 
f1-score (\autoref{eq:f1_score}). However, the TP, FP and FN stands for the sum of the metrices for all classes.  
\begin{equation}\label{eq:micro_f1}
	\text{micro-f1} = \frac{\text{TP}}{\text{TP}+\frac{1}{2}(\text{FP+FN})}
\end{equation}
This shows that f1-score views each observation points equally important. This might cause a bias of measurement 
with imbalanced datasets. With f1-score, larger classes with more observation points will have a larger impact on 
the f1-score.\\[10pt]
For the task, both macro-f1 and micro-f1 will be used for the evaluation on the testing dataset. A comparison and 
discussion of the two metrices will be explained more in \autoref{sec:discussion}.
%----------------------------------------------------------------------------------------
%	METHOD
%----------------------------------------------------------------------------------------
\section{Method}
In this section, the methods of this particular task will be explained, including introduction of data, building 
features for the models, training and predicting classifiers and generating the feature importance table. 
Classifiers and other used tools are introduced and explained in \autoref{sec:domain_knowledge}.
\subsection{Data}
Data used for this task is from covid-19-data from Our World of Data. It is online in a github repository, making 
it easy to access. The data is loaded into the local database by the \emph{request} package of Python.\\[10pt]
The data consists of 67 columns and 248 countries with 236386 rows in total. Attribute \emph{location} and 
\emph{date} together define each unique row of data. The data is still being updated. 
A summary table of the data is shown 
in \autoref{tab:data_summary}.
\begin{table}
	\caption{Data Summary Table}
	\centering
	\begin{tabular}{lr}
		\toprule
		\textbf{Information} & \textbf{Description} \\
		\midrule
		number of columns & 67 \\
		number of rows & 236386 \\
		number of countries & 248 \\
		key & \{location:date\}\\
		starting date & 01.Jan.2020\\
		\bottomrule
	\label{tab:data_summary}
	\end{tabular}
\end{table}
\subsection{Building Features}
The process of building features includes data cleaning and feature selection, and label preparation. Data cleaning 
and feature selecting is crucial for the accuracy of models. Bad data cleaning can lead to biased results or 
bad model performances. Relevant attributes will be selected as features to be put into the classifiers, i.e., 
the models. Finally, since this is a supervised learning task, the label should be prepared for model training.\\[10pt]
In the data cleaning phase, the goal is to get a table of one row per country, i.e., each row represents the 
information of a country. The label is defined as the attribute, \emph{total\_cases\_per\_million}. To achieve this, 
relavent attributes are first selected for model training. For this task, eleven attributes that is speculated to 
affect the number of COVID cases are selected from the raw data. The selected attributes are listed in 
\autoref{tab:attributes}. Countries with less than 200 rows of data is then removed. Since each row corresponds 
to one date, it is not ideal to use the data of a country if less than 200 days of data are recorded. Among the 
selected attributes, \emph{aged\_65\_older} and \emph{aged\_70\_older} are divided by \emph{population} into 
\emph{aged\_65\_older\_percentage} and \emph{aged\_70\_older\_percentage} respectively so that the models are 
trained on the percentage of elder people instead of the total number. Except for the attribute 
\emph{people\_fully\_vaccinated\_per\_hundred}, all the other attributes have a single value throughout the dates 
for each country. However, the attribute \emph{people\_fully\_vaccinated\_per\_hundred} and the label attribute 
\emph{total\_cases\_per\_million} is accumulated day by day. In this case, the data of the latest date is used as 
the feature value for each country. By doing this, the model will be able to generate the feature importance 
table according to how all features affect the number of total cases per million in each country. After data 
cleaning and feature selection, 194 countries/rows and 11 features are left for model training.\\[10pt]
\begin{table}
	\caption{Data Summary Table}
	\centering
	\begin{tabular}{ll}
		\toprule
		\textbf{Selected Attributes} &  \\
		\midrule
		aged\_65\_older & cardiovasc\_death\_rate \\
		aged\_70\_older & diabetes\_prevalence \\
		gdp\_per\_capita & hospital\_beds\_per\_thousand \\
		population\_density & human\_development\_index \\
		life\_expectancy & median\_age \\
		\multicolumn{2}{l}{people\_fully\_vaccinated\_per\_hundred} \\
		\bottomrule
	\label{tab:attributes}
	\end{tabular}
\end{table}
After data cleaning and feature selection, label preparation is performed. The attribute \emph{total\_cases\_per\_million} 
is categorised into four levels of severity. The interval of the categorisation is shown in \autoref{tab:label_interval}. 
Apparently, there is no country with over 700'000 cases per million. 
\begin{table}
	\caption{Label Interval for Label Preparation}
	\centering
	\begin{tabular}{ll}
		\toprule
		\textbf{Level} & \textbf{Interval} \\
		\midrule
		0 & 0 - 50'000 \\
		1 & 50'000 - 200'000 \\
		2 & 200'000 - 400'000 \\
		3 & 400'000 - 700'000\\
		\bottomrule
	\label{tab:label_interval}
	\end{tabular}
\end{table}
[0, 50000, 200000, 400000, 700000]
\subsection{Classifiers}
Classifiers are used for classification tasks. Due to the small dimension of the dataset, several changes are made to 
adapt the data size. The data is not splitted into training, validation and testing datasets, but only training and 
testing only. In this task, the training-testing split will be 80\% and 20\% respectively. Besides, it is not recommended 
to use models with too many parameters since it might have to higher chance of overfitting. Thus, simple models are 
picked out for this task including logistic regression, linear perceptron and XGBoost. These models can be easily 
applied to the dataset with \emph{sklearn} from Python. For hyperparameter selection, grid search cross validation 
is used. A summary of models used for the task and the best performing hyperparameters chosen by applying grid search 
cross validation are listed out in \autoref{tab:model_summary}.
\begin{table}
	\caption{Model Summary}
	\centering
	\begin{tabular}{lr}
		\toprule
		\textbf{Model} & \textbf{Details} \\
		\toprule
		\textbf{Logistic Regression} & \\ 
		penalty & l2 \\
		solver & newton-cg \\
		\midrule
		\textbf{Linear Perceptron} & \\
		tolerance & 0.001 \\
		random state & 0 \\
		\midrule
		\textbf{XGBoost} & \\
		learning rate & 0.1\\
		loss & deviance \\
		max depth & 3 \\
		n\_estimators & 100 \\
		random state & 21 \\
	\bottomrule
	\label{tab:model_summary}
	\end{tabular}
\end{table}
\subsection{Feature Importance Table}
The feature importance is generated automatically via the trained model. It shows weight of each feature. The weight can 
be thought of as how significant each feature effects the classification accuracy. The more positive the feature importance 
score, the more the feature helps reduce the loss while training. However, if the feature importance is negative, the feature 
increases the loss while training.
\section{Results}
By applying the test dataset on the model, the performance of the models can be evaluated. The accuracy summary of each 
model and the feature importance table from the best performing model is shown in \autoref{tab:acc_summary} and 
\autoref{tab:feature_importance}. Among the models, XGBoost has the best performance. Among the features, the model suggests 
that human development index\footnote{According to \href{https://github.com/owid/covid-19-data/tree/master/public/data}{Our World In Data}, 
human development index is: A composite index measuring average achievement in three basic dimensions of human 
development—a long and healthy life, knowledge and a decent standard of living. Values for 2019, imported from 
\url{http://hdr.undp.org/en/indicators/137506}}, life expectancy\footnote{According to 
\href{https://github.com/owid/covid-19-data/tree/master/public/data}{Our World In Data}, Life expectancy is: Life 
expectancy at birth in 2019}, population density, hospital beds per thousand 
and gdp per capita are the top five items that affect the covid cases per milliom in a country.
\begin{table}
	\caption{Model Accuracy Summary}
	\centering
	\begin{tabular}{lrrr}
		\toprule
		\textbf{Model} & \textbf{Accuracy} & \textbf{Micro-f1} & \textbf{Macro-f1} \\
		\midrule
		Logistic Regression & 64.10\% & 0.6410 & 0.5863 \\
		Linear Perceptron & 48.72\% & 0.4872 & 0.2976 \\
		XGBoost & 76.92\% & 0.7692 &0.7749 \\
		\bottomrule
	\label{tab:acc_summary}
	\end{tabular}
\end{table}
\begin{table}
	\caption{Feature Importance from XGBoost}
	\centering
	\begin{tabular}{lll}
		\toprule
		& \textbf{Feature} & \textbf{Importance} \\
		\midrule
		1&human\_development\_index & 0.462551 \\
		2& life\_expectancy& 0.462551\\
		3& population\_density& 0.138737\\
		4& hospital\_beds\_per\_thousand& 0.082238\\
		5& gdp\_per\_capita& 0.065485\\
		6& people\_fully\_vaccinated\_per\_hundred& 0.059118\\
		7& cardiovasc\_death\_rate& 0.045620\\
		8& aged\_70\_older\_percentage& 0.033860\\
		9& median\_age& 0.029610\\
		12& diabetes\_prevalence& 0.026468\\
		11& aged\_65\_older\_percentage& 0.025800\\
		\bottomrule
	\label{tab:feature_importance}
	\end{tabular}
\end{table}
%----------------------------------------------------------------------------------------
%	DISCUSSION
%----------------------------------------------------------------------------------------
\section{Dicussion}\label{sec:discussion}

%----------------------------------------------------------------------------------------
%	CONCLUSION
%----------------------------------------------------------------------------------------
\section{Conclusion}
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={Bibliography}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
